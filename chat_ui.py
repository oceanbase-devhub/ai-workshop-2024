import os
import dotenv

dotenv.load_dotenv()

from typing import Iterator, Union
from rag.doc_rag import doc_rag_stream

import streamlit as st
from langchain_core.messages import BaseMessageChunk


class StreamResponse:
    """
    StreamResponse is a class that helps to stream the response from the chatbot.
    """

    def __init__(self, chunks: Iterator[BaseMessageChunk] = []):
        self.chunks = chunks
        self.__whole_msg = ""

    def generate(
        self,
        *,
        prefix: Union[str, None] = None,
        suffix: Union[str, None] = None,
    ) -> Iterator[str]:
        if prefix:
            yield prefix
        for chunk in self.chunks:
            self.__whole_msg += chunk.content
            yield chunk.content
        if suffix:
            yield suffix

    def get_whole(self) -> str:
        return self.__whole_msg


st.set_page_config(
    page_title="RAG æ™ºèƒ½é—®ç­”åŠ©æ‰‹",
    page_icon="demo/ob-icon.png",
)
st.title("ğŸ’¬ æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
st.caption("ğŸš€ ä½¿ç”¨ OceanBase å‘é‡æ£€ç´¢ç‰¹æ€§å’Œå¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›æ„å»ºçš„æ™ºèƒ½é—®ç­”æœºå™¨äºº")
st.logo("demo/logo.png")

env_table_name = os.getenv("TABLE_NAME", "corpus")
env_llm_base_url = os.getenv("LLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4/")

with st.sidebar:
    st.subheader("ğŸ”§ è®¾ç½®")
    st.text_input(
        "è¡¨å",
        value=env_table_name,
        disabled=True,
        help="ç”¨äºå­˜æ”¾æ–‡æ¡£åŠå…¶å‘é‡æ•°æ®çš„è¡¨åï¼Œç”¨ç¯å¢ƒå˜é‡ TABLE_NAME è¿›è¡Œè®¾ç½®",
    )
    if env_llm_base_url == "https://open.bigmodel.cn/api/paas/v4/":
        llm_model = st.selectbox(
            "é€‰ç”¨çš„å¤§è¯­è¨€æ¨¡å‹",
            ["glm-4-flash", "glm-4-air", "glm-4-plus", "glm-4-long"],
            index=0,
        )
    history_len = st.slider(
        "èŠå¤©å†å²é•¿åº¦",
        min_value=0,
        max_value=25,
        value=3,
        help="èŠå¤©å†å²é•¿åº¦ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£",
    )
    search_docs = st.checkbox(
        "è¿›è¡Œæ–‡æ¡£æ£€ç´¢",
        True,
        help="æ£€ç´¢æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ï¼Œå¦åˆ™åªä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å›ç­”é—®é¢˜",
    )
    oceanbase_only = st.checkbox(
        "ä»…é™ OceanBase ç›¸å…³é—®é¢˜",
        True,
        help="å‹¾é€‰åæœºå™¨äººåªä¼šå›ç­” OceanBase æœ‰å…³çš„é—®é¢˜",
    )
    rerank = st.checkbox(
        "è¿›è¡Œæ–‡æ¡£é‡æ’åº",
        False,
        help="ä½¿ç”¨ BGE-M3 å¯¹æ£€ç´¢çš„æ–‡æ¡£è¿›è¡Œé‡æ’åºä»¥æé«˜æ£€ç´¢ç»“æœçš„è´¨é‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ…¢çš„è¿‡ç¨‹ï¼Œè¯·ä»…åœ¨æœ‰éœ€è¦æ—¶ä½¿ç”¨",
    )

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}
    ]

avatar_m = {
    "assistant": "demo/ob-icon.png",
    "user": "ğŸ§‘â€ğŸ’»",
}

for msg in st.session_state.messages:
    st.chat_message(msg["role"], avatar=avatar_m[msg["role"]]).write(msg["content"])


def remove_refs(history: list[dict]) -> list[dict]:
    """
    Remove the references from the chat history.
    This prevents the model from generating its own reference list.
    """
    return [
        {
            "role": msg["role"],
            "content": msg["content"].split("æ ¹æ®å‘é‡ç›¸ä¼¼æ€§åŒ¹é…æ£€ç´¢")[0],
        }
        for msg in history
    ]


if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨æƒ³å’¨è¯¢çš„é—®é¢˜..."):
    st.chat_message("user", avatar=avatar_m["user"]).write(prompt)

    history = st.session_state["messages"][-history_len:] if history_len > 0 else []

    it = doc_rag_stream(
        query=prompt,
        chat_history=remove_refs(history),
        universal_rag=not oceanbase_only,
        rerank=rerank,
        llm_model=llm_model,
        search_docs=search_docs,
    )

    with st.status("å¤„ç†ä¸­...", expanded=True) as status:
        for msg in it:
            if not isinstance(msg, str):
                status.update(label="æ€è€ƒå®Œæ¯•ï¼")
                break
            st.write(msg)

    res = StreamResponse(it)

    st.session_state.messages.append({"role": "user", "content": prompt})

    st.chat_message("assistant", avatar=avatar_m["assistant"]).write_stream(
        res.generate()
    )

    st.session_state.messages.append({"role": "assistant", "content": res.get_whole()})
